---
title: "Intrinsic 增益为零：TD3 中边界退化问题的诊断记录"
summary: "围绕真实研究起因展开：intrinsic reward 增益接近零，随后发现 action 边界退化；通过 state_norm 缓解状态量纲失衡后，动作退化问题得到缓解。"
date: 2026-02-16
slug: "td3-intrinsic-action-freeze-zh"
canonicalSlug: "td3-intrinsic-action-freeze"
tags:
  - td3
  - intrinsic reward
  - reinforcement learning
  - risk modeling
locale: zh
cover: "fig_action_gap_phase"
---

import figAction from '@/assets/blog/td3-intrinsic-toy/fig_action_gap_phase.png';
import figGate from '@/assets/blog/td3-intrinsic-toy/fig_gate_and_scale.png';
import figTransport from '@/assets/blog/td3-intrinsic-toy/fig_gate_transport_scale.png';
import figAblation from '@/assets/blog/td3-intrinsic-toy/fig_ablation_matrix.png';

## 研究起因

这篇文章回到你最初的真实研究问题：

1. 在 total reward 里额外加入 intrinsic reward 后，整体增益几乎为零；
2. 排查时发现 action 空间长期退化到边界；
3. 引入 `state_norm` 处理状态各维度量纲差异后，action 退化有所缓解。

所以核心并不是“intrinsic 在理论上是否有效”，而是：

**为什么在当前训练系统里 intrinsic 看起来无效，以及 state_norm 为什么会起作用。**

## 观察到的现象

训练日志反复出现同一模式：

- intrinsic 指标非零；
- critic value 在变化；
- policy action 长时间贴边；
- 行为层面的改进远小于预期。

也就是“值在动，策略不太动”。

## 问题建模

总奖励为

$$
r_t^{tot} = r_t^{env} + w\,r_t^{int},
$$

actor 输出常写成

$$
a = a_{max}\tanh(u_\theta(s)).
$$

从 pre-activation 到 action 的传输门控为

$$
\kappa(u)=\frac{\partial a}{\partial u}=a_{max}(1-\tanh^2(u)).
$$

当 $|u|$ 变大时，$\kappa(u)$ 接近 0，actor 的有效更新会显著变弱。

<img src={figGate.src} alt="Gate and scale effects" />

*图：边界饱和会压低门控，导致动作更新传输不足。*

## 机制假设

我们对该现象的机制解释是：

- state 各维度量纲差异过大；
- 大量纲维度主导 actor pre-activation；
- pre-activation 更容易进入饱和区；
- action 更容易长期贴边；
- intrinsic 虽能影响 value，但难以有效传导到可见行为变化。

这就解释了“intrinsic 加了但增益接近零”的表象。

## 为什么 intrinsic 看起来“无效”

行为层面看不到增益，常是两道门槛同时存在：

- **排序门槛**：intrinsic 对动作偏好的差异不足以跨过局部 action gap；
- **传输门槛**：即便有差异，actor 侧梯度在饱和下也传不动。

前者决定“偏好有没有翻转”，后者决定“参数能不能跟上”。

<img src={figAction.src} alt="Action-gap phase boundary" />

*图：action-gap 视角下，仅有 value 偏移并不自动等于策略行为改变。*

## 干预：加入 State Normalization

我们采用

$$
\tilde{s}_i = \frac{s_i-\mu_i}{\sigma_i+\epsilon},
$$

在 replay 状态上维护运行统计。这个操作的目的不是形式化预处理，而是直接缓解 pre-activation 的量纲失衡与饱和风险。

实际表现是：

- 持续贴边动作明显减少；
- intrinsic shaping 对 policy 行为的可见影响变强。

## 修复后证据

<img src={figTransport.src} alt="Transport Gate vs Pre-Activation Scale" />

*图：控制量纲失衡后，门控衰减显著缓和。*

<img src={figAblation.src} alt="Toy Ablation: Policy Motion vs Stability" />

*图：toy ablation 显示归一化相关设置能提升动作变化，同时保持可接受的 value 稳定性。*

对应到你的原始结论：

- `state_norm` 不是万能修复；
- 但它确实命中了一个关键瓶颈（边界退化）；
- 因而 intrinsic reward 才更有机会转化为策略增益。

## 这个故障模式的排查清单

当 intrinsic 增益接近零时，建议按以下顺序排查：

1. action 边界占用率；
2. state 各维量纲比值；
3. actor pre-activation 分布与饱和率；
4. actor 漂移速度与 critic 漂移速度；
5. 固定 seed 预算下对比有无 state_norm。

这通常比盲目调 intrinsic 权重更接近真实根因。

## 结论

本篇回到最初问题本身：

- intrinsic reward 增益为零并非必然代表 intrinsic 设计错误；
- 在你的场景里，关键根因之一是状态量纲失衡引起的 action 边界退化；
- `state_norm` 缓解该退化后，策略行为层面的改进开始出现。

核心经验是：**先确认策略传输链路没有被量纲与饱和阻断，再评估 intrinsic 机制本身。**
