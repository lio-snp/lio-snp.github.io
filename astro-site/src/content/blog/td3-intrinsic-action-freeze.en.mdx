---
title: "Intrinsic Reward Gave Zero Gain: Diagnosing Boundary Collapse in TD3"
summary: "A case-study note: intrinsic reward looked useless at first, because actions collapsed to boundaries; state normalization mitigated scale mismatch and partially recovered policy movement."
date: 2026-02-16
slug: "td3-intrinsic-action-freeze-en"
canonicalSlug: "td3-intrinsic-action-freeze"
tags:
  - td3
  - intrinsic reward
  - reinforcement learning
  - risk modeling
locale: en
cover: "fig_action_gap_phase"
---

import figAction from '@/assets/blog/td3-intrinsic-toy/fig_action_gap_phase.png';
import figGate from '@/assets/blog/td3-intrinsic-toy/fig_gate_and_scale.png';
import figTransport from '@/assets/blog/td3-intrinsic-toy/fig_gate_transport_scale.png';
import figAblation from '@/assets/blog/td3-intrinsic-toy/fig_ablation_matrix.png';

## Research Origin

This note is about the exact issue we saw in our RL pipeline:

1. adding intrinsic reward to total reward gave almost **zero gain**,
2. then we found policy actions degenerating to boundary values,
3. after adding `state_norm` (to fix feature-scale mismatch across state dimensions), boundary degeneration was **partially relieved**.

So the core question is not "does intrinsic reward work in theory?" but:

**why did intrinsic look ineffective in our actual training loop, and why did state normalization help?**

## What We Observed

The training trace repeatedly showed this pattern:

- intrinsic statistics were non-zero,
- critic values moved,
- action outputs stayed near limits for long stretches,
- policy behavior changed much less than expected.

In other words, we had a value-side reaction without a clear action-side reaction.

## Problem Setup

We optimize with

$$
r_t^{tot} = r_t^{env} + w\,r_t^{int},
$$

and deterministic actor output

$$
a = a_{max}\tanh(u_\theta(s)).
$$

The effective transport gate from pre-activation to action is

$$
\kappa(u) = \frac{\partial a}{\partial u}=a_{max}(1-\tanh^2(u)).
$$

When $|u|$ gets large, $\kappa(u)$ approaches zero, so actor-side updates become weak even if critic-side signals are non-zero.

<img src={figGate.src} alt="Gate and scale effects" />

*Figure: boundary saturation implies small transport gate and weak action updates.*

## Mechanism Hypothesis

Our diagnosis is a chain:

- state dimensions had very different scales,
- large-scale dimensions dominated actor pre-activations,
- pre-activations became easier to saturate,
- actions stuck near boundaries,
- intrinsic reward could still move values, but could not effectively move behavior.

This explains why "intrinsic added but no gain" can happen without contradiction.

## Why Intrinsic Appeared Useless

Two conditions can make intrinsic gains invisible at behavior level:

- **ranking condition**: intrinsic contrast does not overcome local action gap,
- **transport condition**: even with contrast, actor gradient transport is too weak under saturation.

The first is about action preference ordering; the second is about whether policy parameters can actually move.

<img src={figAction.src} alt="Action-gap phase boundary" />

*Figure: action-gap view. Value shift alone is insufficient unless local ranking margin is crossed.*

## Intervention: State Normalization

We introduced state normalization as

$$
\tilde{s}_i = \frac{s_i-\mu_i}{\sigma_i+\epsilon},
$$

with running statistics on replay states. The target is not cosmetic preprocessing; it directly reduces pre-activation imbalance and lowers saturation risk.

Practically, this changed training behavior in two ways:

- fewer persistent boundary actions,
- better policy responsiveness to intrinsic shaping.

## Evidence After the Fix

<img src={figTransport.src} alt="Transport Gate vs Pre-Activation Scale" />

*Figure: when scale mismatch is controlled, transport gate decay is much less severe.*

<img src={figAblation.src} alt="Toy Ablation: Policy Motion vs Stability" />

*Figure: toy ablation summary. Normalization-related settings increase action movement while keeping value variance manageable.*

Interpretation aligned with our logs:

- `state_norm` does not magically solve everything,
- but it directly addresses one key bottleneck (boundary collapse),
- therefore intrinsic reward has a better chance to produce observable policy gains.

## Practical Checklist for This Failure Mode

When intrinsic gain is near zero, check in this order:

1. action boundary occupancy rate,
2. per-dimension state scale ratio,
3. actor pre-activation distribution and saturation ratio,
4. actor drift vs critic drift speed,
5. before/after normalization comparison under same seed budget.

This sequence is closer to the real debugging path than tuning intrinsic weight alone.

## Conclusion

The original research problem remains the center:

- intrinsic reward looked useless,
- root cause included action boundary degeneration under state-scale mismatch,
- `state_norm` alleviated degeneration and partially recovered useful policy movement.

So the key lesson is: **before doubting intrinsic design, first verify whether policy transport is blocked by representation scale and saturation.**
